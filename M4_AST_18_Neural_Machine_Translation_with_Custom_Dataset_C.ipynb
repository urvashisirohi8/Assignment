{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urvashisirohi8/Assignment/blob/main/M4_AST_18_Neural_Machine_Translation_with_Custom_Dataset_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 18: Natural Language Processing - (Translate human readable dates to machine readable dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu26Vq9jDTpj"
      },
      "source": [
        "### Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "*  perform seq2seq translation\n",
        "*  use attention architecture for machine translation tasks\n",
        "*  Visualize the parts of the input to which every output pays attention to, while doing the translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement\n",
        "\n",
        "\n",
        "**Translating human readable dates into machine readable dates**\n",
        "\n",
        "The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. However, language translation requires massive datasets and usually takes days of training on GPUs. To give you a place to experiment with these models even without using massive datasets, we will instead use a simpler \"date translation\" task.\n",
        "\n",
        "\n",
        "The network will input a date written in a variety of possible formats (e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\") and translate them into standardized, machine readable dates (e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"). We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD.\n",
        "\n",
        "You will build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). You will do this using an attention model, one of the most sophisticated sequence to sequence models."
      ],
      "metadata": {
        "id": "dy9g6CKKQZcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Dataset\n",
        "\n",
        "We will train the model on a dataset of 50000 human readable dates and their equivalent, standardized, machine readable dates."
      ],
      "metadata": {
        "id": "vCeM4b-TTDo9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KzNzdZoQeBLz"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_18_Neural_Machine_Translation_with_Custom_Dataset_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/dates_dataset.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVM_u_Czx_jZ"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjGGk7W-nY0x"
      },
      "source": [
        "import torch,os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random,os,sys\n",
        "# The date and time functionality provided by Babel lets you format\n",
        "# standard Python datetime, date and time objects and work with timezones.\n",
        "from babel.dates import format_date\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dates dataset"
      ],
      "metadata": {
        "id": "GY5zm0zNURO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = pd.read_csv(\"dates_dataset.csv\")\n",
        "dates.columns = [\"human_readable\", \"machine_readable\"]\n",
        "dates.head()"
      ],
      "metadata": {
        "id": "IKldiEgjEDeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of the dataset\n",
        "dates.shape"
      ],
      "metadata": {
        "id": "7A_5xckvWsuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosidering 40000 dates for training set\n",
        "n_datas = 40000\n",
        "dt_train = dates.iloc[:n_datas]\n",
        "print(dt_train.shape)"
      ],
      "metadata": {
        "id": "ZRVfVbSc5gk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function for Human readable and machine readable dates\n",
        "\n",
        "We are loading the below:\n",
        "\n",
        "**X:** a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via human_vocab. Each date is further padded to  30 values (human_readable_length) with a special character (< pad >).\n",
        "\n",
        "**Y:** a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in machine_vocab.\n",
        "\n",
        "**Xoh:** one-hot version of X, the \"1\" entry's index is mapped to the character to human_vocab. Xoh.shape = (no. of dates, 30, len(human_vocab)). Here len(human_vocab) = 36\n",
        "\n",
        "**Yoh:** one-hot version of Y, the \"1\" entry's index is mapped to the character to machine_vocab. Yoh.shape = (no. of dates, 10, len(machine_vocab)). Here, len(machine_vocab) = 11 since there are 11 characters ('-' as well as 0-9).\n"
      ],
      "metadata": {
        "id": "OieT0WXwE5tt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fb0rmI7bUPM"
      },
      "source": [
        "# Let's preprocess the data and map the raw text data into the index values\n",
        "def transform(human_readable, machine_readable, human_vocab, machine_vocab, human_readable_length):\n",
        "\n",
        "    # For every item(x) in human readable we are getting corresponding keys from human_vocab\n",
        "    # If the key doesn't exist we return <unk> token\n",
        "    # Converting dates which are in string format to integer\n",
        "    # will return a list of indexes based on a string and vocabulary\n",
        "    X = list(map(lambda x: human_vocab.get(x, '<unk>'), human_readable))\n",
        "\n",
        "    # will return a list of indexes based on a string and vocabulary\n",
        "    # human_readable_length=30 (which we assume is the maximum length of the human readable date\n",
        "    # if we get a longer input, we would have to truncate it)\n",
        "    # If the input is less than the maximum length we add the padding\n",
        "    # We also add two special chars, <unk> for unknown characters, and <pad> to add padding at the end\n",
        "    if len(X) < human_readable_length:\n",
        "        X += [human_vocab['<pad>']] * (human_readable_length - len(X))\n",
        "    elif len(X) > human_readable_length:\n",
        "        X = X[:30]\n",
        "\n",
        "    # Y=10 (since \"YYYY-MM-DD\" is 10 characters long)\n",
        "    # If the key doesn't exist we return <unk> token\n",
        "    Y = list(map(lambda x: machine_vocab.get(x, '<unk>'), machine_readable)) # len(Y) is always 10, because the format is YYYY-MM-DD\n",
        "\n",
        "    def zcs(length, idx):\n",
        "        ret = np.zeros(length)\n",
        "        ret[idx] = 1\n",
        "        return ret\n",
        "\n",
        "    Xoh = np.array(list(map(partial(zcs, len(human_vocab)), X)), dtype=np.float32)\n",
        "    Yoh = np.array(list(map(partial(zcs, len(machine_vocab)), Y)), dtype=np.float32)\n",
        "\n",
        "    return Xoh, Yoh, {'human_readable':human_readable, 'machine_readable':machine_readable}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dataset:** a list of tuples of (human readable date, machine readable date).\n",
        "\n",
        "**human_vocab:** a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
        "\n",
        "For Example **human_vocab** looks like the below dictionary:\n",
        "\n",
        "{' ': 0,\n",
        " '.': 1,\n",
        " '/': 2,\n",
        " '0': 3,\n",
        " '1': 4,\n",
        " '2': 5,\n",
        " '3': 6,\n",
        " '4': 7,\n",
        " '5': 8,\n",
        " '6': 9,\n",
        " '7': 10,\n",
        " '8': 11,\n",
        " '9': 12,\n",
        " '<pad>': 36,\n",
        " '<unk>': 35,\n",
        " 'a': 13,\n",
        " 'b': 14,\n",
        " 'c': 15,\n",
        " 'd': 16,\n",
        " 'e': 17,\n",
        " 'f': 18,\n",
        " 'g': 19,\n",
        " 'h': 20,\n",
        " 'i': 21,\n",
        " 'j': 22,\n",
        " 'l': 23,\n",
        " 'm': 24,\n",
        " 'n': 25,\n",
        " 'o': 26,\n",
        " 'p': 27,\n",
        " 'r': 28,\n",
        " 's': 29,\n",
        " 't': 30,\n",
        " 'u': 31,\n",
        " 'v': 32,\n",
        " 'w': 33,\n",
        " 'y': 34}\n",
        "\n",
        "**machine_vocab:** a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with human_vocab.\n",
        "\n",
        "For Example **machine_vocab** looks like the below dictionary:\n",
        "\n",
        "{'-': 0,\n",
        " '0': 1,\n",
        " '1': 2,\n",
        " '2': 3,\n",
        " '3': 4,\n",
        " '4': 5,\n",
        " '5': 6,\n",
        " '6': 7,\n",
        " '7': 8,\n",
        " '8': 9,\n",
        " '9': 10}\n",
        "\n",
        "\n",
        "\n",
        "**inv_machine_vocab:** the inverse dictionary of machine_vocab, mapping from indices back to characters.\n",
        "\n",
        "For Example **inv_machine_vocab:** looks like the below dictionary:\n",
        "\n",
        "{0: '-',\n",
        " 1: '0',\n",
        " 2: '1',\n",
        " 3: '2',\n",
        " 4: '3',\n",
        " 5: '4',\n",
        " 6: '5',\n",
        " 7: '6',\n",
        " 8: '7',\n",
        " 9: '8',\n",
        " 10: '9'}"
      ],
      "metadata": {
        "id": "ArGPZ8MVfzOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Dataset Class for reading the dates data"
      ],
      "metadata": {
        "id": "bGiJ0LPxFl8H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycRSVcD8njZZ"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, transform, dates = dt_train, n_datas=40000, seed=12345):\n",
        "\n",
        "        # Transformations defined above\n",
        "        self.transform = transform\n",
        "\n",
        "        # Human and machine readable dates(40000)\n",
        "        self.human_readable = dt_train['human_readable']\n",
        "        self.machine_readable = dt_train['machine_readable']\n",
        "\n",
        "        self.human_readable_length = 30 # the maximum length of input sequence is 30\n",
        "        self.human_vocab = set()\n",
        "        self.machine_vocab = set()\n",
        "        self.dataset = []\n",
        "        for i in tqdm(range(n_datas)):\n",
        "            human_readable, machine_readable = dt_train.iloc[i][\"human_readable\"].rstrip()[1:-1], dt_train.iloc[i][\"machine_readable\"].rstrip()[2:-1]\n",
        "            # print(human_readable, machine_readable)\n",
        "\n",
        "            self.dataset.append((human_readable, machine_readable))\n",
        "            self.human_vocab.update(tuple(human_readable))\n",
        "            self.machine_vocab.update(tuple(machine_readable))\n",
        "\n",
        "        self.human_vocab = dict(zip(sorted(self.human_vocab) + ['<unk>', '<pad>'], list(range(len(self.human_vocab) + 2))))\n",
        "\n",
        "        # Mapping indexes back to the characters\n",
        "        self.inv_machine_vocab = dict(enumerate(sorted(self.machine_vocab)))\n",
        "\n",
        "        self.machine_vocab = {v:k for k, v in self.inv_machine_vocab.items()}\n",
        "        print(f\"human_vocab : {self.human_vocab}\")\n",
        "        print(f\"machine_vocab : {self.machine_vocab}\")\n",
        "        print(f\"inverse_vocab : {self.inv_machine_vocab}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        human_readable, machine_readable = self.dataset[idx] # dataset is a list of tuples\n",
        "        return self.transform(human_readable, machine_readable, self.human_vocab, self.machine_vocab, self.human_readable_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data using pytorch data loader"
      ],
      "metadata": {
        "id": "Zv-8VS7MF-SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset(dates = dt_train, transform = transform)"
      ],
      "metadata": {
        "id": "xE4_dl1xzhHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asuxbx_aBsp5"
      },
      "source": [
        "**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features.\n",
        "\n",
        "1. Batching the data - `batch_size`, which denotes the number of samples contained in each generated batch. The Machine learning dataset can be really large. Hence we cannot often load the entire data into the memory. Hence neural network training is done by loading small batches (commonly called minibatch) of data and using it to update the learnable parameters (weights and biases) of the model.\n",
        "\n",
        "2. Shuffling the data - If set to `shuffle=True`, we will get a new order of exploration at each pass. Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike. By performing it will eventually make our model more robust.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                        batch_size=100,\n",
        "                                        shuffle=False,\n",
        "                                        num_workers=0,\n",
        "                                        collate_fn=None)"
      ],
      "metadata": {
        "id": "6ibh_kinHRfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate through the dataset and print the shape of the input and output batches"
      ],
      "metadata": {
        "id": "PBlhy_IRibI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch_x in enumerate(dataloader):\n",
        "    print(batch_x[0].shape, batch_x[0])\n",
        "    print(batch_x[1].shape, batch_x[1])\n",
        "    if i >= 1:\n",
        "        break"
      ],
      "metadata": {
        "id": "bbl9g_xcHL9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Machine Translation (NMT) with Attention\n",
        "\n",
        "\n",
        "NMT is a problem where we process an input sequence to produce an output sequence — that is, a sequence-to-sequence (seq2seq) problem. Specifically, the many-to-many type, with a sequence of several elements both at the input and at the output, and the encoder-decoder architecture for recurrent neural networks is the standard method.\n",
        "\n",
        "Initially machine translation (MT) problems were faced using statistical approaches, based mainly on Bayes probabilities. But when neural networks became more powerful and popular, researchers began to explore the capabilities of this technology and new solutions were found. It is called neural machine translation (NMT).\n",
        "\n",
        "The model based on RNNs has a serious problem when working with long sequences because the information of the first tokens is lost or diluted as more tokens are processed. The context vector has been given the responsibility of encoding all of the information in a given source sentence into a vector of few hundred elements. It made it challenging for the models to deal with long sentences.\n",
        "\n",
        "In this paper https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "\n",
        "They introduce a technique called **attention**, which highly improved the quality of machine-translation systems. “**Attention** allows the model to focus on the relevant parts of the input sequence as needed, accessing all the past hidden states of the encoder, instead of just the last one”.\n",
        "\n",
        "At each decoding step, the decoder gets to look at any particular state of the encoder and can selectively pick out specific elements from that sequence to produce the output."
      ],
      "metadata": {
        "id": "trChpcm7LNTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![seq2seq.webp](data:image/webp;base64,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)\n",
        "\n",
        "**Note:**\n",
        "\n",
        "1. Refer to the following [link](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) to understand more in detail about the Attention Machanism.\n",
        "\n",
        "2.  Refer to the following [link](https://betterprogramming.pub/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb) to understand Encoder-Decoder Model and the Attention Mechanism.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xdJRckK9KNG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A BASIC LSTM-BASED ENCODER & DECODER MODEL\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/LSTM_1.png\" width=\"800\" height=\"500\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "JO_fVzEA7yA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Encoder Architecture (Bi-directional LSTM)\n",
        "\n",
        "Encoder cell are simple RNN cell (LSTM or GRU can be used for better performance) which takes the input vectors. The input is taken as a single character at each and every time stamp but the out is not taken at each state. Output of each encoder cell is rejected and internal state are used to generate encoder vector.\n",
        "\n",
        "\n",
        "\n",
        "The input tokens/characters one hot vectors are fed one at a time into the forward LSTM network and the backward LSTM network simultaneously. Then the hidden states\n",
        "emitted by both networks at each timestep are concentrated to form a single hidden state.\n",
        "\n",
        "During the decoding process, the context vector at each timestep is\n",
        "calculated through the attention layer. Then the decoder predicts future observations one by one by making use of the context vector and the current hidden state."
      ],
      "metadata": {
        "id": "awDs4pVUOZxi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNZb1nQsbaY7"
      },
      "source": [
        "# Bi-directional LSTM as Encoder Model\n",
        "# We give the human readable vocab to the encoder and it should generate a vector that represents the inputs\n",
        "# Encoder processes the input and provides the context vector\n",
        "# (Last time step's hidden state + Last time step's cell state) for the decoder.\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, in_features=36, hidden_size=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Input features (human vocabulary) = 36 and hidden size = 64\n",
        "        self.linear = torch.nn.Linear(in_features=in_features, out_features=hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        # Bi-directional LSTM\n",
        "        self.lstm = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        self.lstm.flatten_parameters()\n",
        "        x, _ = self.lstm(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Decoder Architecture (Single LSTM)\n",
        "\n",
        "Same as encoder decoder also consist of series of RNN/LSTM. Here we are taking an output at each time stamp t.\n",
        "Each RNN/LSTM cell produce output y_t with the previous output y_t-1 and hidden state ht-1. Output y_t is calculated with ht and y_t-1. we use softmax function to determine the output as this prediction is probabilistic."
      ],
      "metadata": {
        "id": "e2b0EVasOeJp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwa8iQoWnrw4"
      },
      "source": [
        "# Decoder Architecture\n",
        "# Now to generate the machine readable dates, we will make another LSTM which is the decoder.\n",
        "# Attention weights are used to specify which characters are needed — when to generate a character/output\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, in_features=128, hidden_size=128):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(input_size=in_features, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=False)\n",
        "        self.linear = torch.nn.Linear(in_features=hidden_size, out_features=11)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        self.lstm.flatten_parameters()\n",
        "        x, (h, c) = self.lstm(x, (h, c))\n",
        "        x = self.linear(x.squeeze())\n",
        "        x = self.softmax(x)\n",
        "        return x, (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Encoder-Attention-Decoder"
      ],
      "metadata": {
        "id": "5a5a6qELOkQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/LSTM_2.png\" width=\"850\" height=\"500\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "xtZx8yvo-z82"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haQF38TAnxIo"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, output_len, use_gpu=True):\n",
        "        super(Model, self).__init__()\n",
        "        self.output_len = output_len\n",
        "        self.use_gpu = use_gpu\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(in_features=36, hidden_size=64)\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(in_features=128, hidden_size=128)\n",
        "\n",
        "        # Attention network\n",
        "        self.linear1 = torch.nn.Linear(in_features=128+128, out_features=10)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.linear2 = torch.nn.Linear(in_features=10, out_features=1)\n",
        "        self.softmax = torch.nn.Softmax(dim=1) # this is important\n",
        "        self.scores_for_paint = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The pre-attention Bi-LSTM goes through 30 time steps; the post-attention LSTM goes through 10 time steps.\n",
        "        # Encoder output is calculated only once\n",
        "        feats = self.encoder(x) #(N, 30, 128)\n",
        "\n",
        "        # First, we initialize zeros for Hidden state and cell state of first decoder unit\n",
        "        # and use gpu\n",
        "        if self.use_gpu:\n",
        "            decoder_h = torch.zeros((1, feats.shape[0], 128)).cuda() #(1, N, 128)\n",
        "            decoder_c = torch.zeros((1, feats.shape[0], 128)).cuda() #(1, N, 128)\n",
        "        else:\n",
        "            decoder_h = torch.zeros((1, feats.shape[0], 128)) #(1, N, 128)\n",
        "            decoder_c = torch.zeros((1, feats.shape[0], 128)) #(1, N, 128)\n",
        "\n",
        "        # Initialize empty list of outputs\n",
        "        outputs = []\n",
        "        self.scores_for_paint = []\n",
        "\n",
        "        # Decoder loop repeats 10 times as output sequence is 10\n",
        "        for _ in range(self.output_len):\n",
        "\n",
        "            # Reshape decoder cell state\n",
        "            feats_2 = decoder_c.transpose(0, 1) #(1, N, 128) --> (N, 1, 128)\n",
        "            feats_2 = feats_2.repeat(1, feats.shape[1], 1) #(N, 1, 128) --> (N, 30, 128), because the max length of feats.shape[1] is 30\n",
        "\n",
        "            # The decoder hidden state and encoder hidden states are added together first before being passed through a linear layer with a tanh activation\n",
        "            feats_in = torch.cat((feats, feats_2), dim=-1) # (N, 30, 128) and (N, 30, 128) --> (N, 30, 256)\n",
        "\n",
        "            # Attention starts here\n",
        "            out = self.tanh(self.linear1(feats_in)) #(N, 30, 256) --> (N, 30, 10)\n",
        "\n",
        "            # These softmaxed scores represent the attention distribution\n",
        "            scores = self.softmax(self.linear2(out)) #(N, 30, 10) --> (N, 30, 1)\n",
        "            # print(scores[0, :, 0])\n",
        "\n",
        "            if not self.training:\n",
        "                self.scores_for_paint.append(scores.squeeze().detach().cpu().numpy())\n",
        "                # print(scores)\n",
        "\n",
        "            # multiplying each encoder hidden state with its softmaxed score (scalar) we obtain the alignment vector\n",
        "            # and sum up the alignment vectors to produce context vector\n",
        "            # A context vector is aggregated information of the alignment vectors from the previous step.\n",
        "            feats_for_decoder = torch.mul(feats, scores).sum(axis=1).unsqueeze(1) #(N, 30, 128) mul (N, 30, 1) --> (N, 30, 128) --> (N, 128) --> (N, 1, 128)\n",
        "            # Attention ends here\n",
        "\n",
        "            # Feed the context vector to the decoder\n",
        "            # Decoder calculates ouput token prediction and returns hidden state, cell state\n",
        "            decoder_out, (decoder_h, decoder_c) = self.decoder(feats_for_decoder, decoder_h, decoder_c) #run lstm only one step, because feats_for_decoder.shape is (N, 1, 128)\n",
        "            outputs.append(decoder_out.unsqueeze(1)) #list of (N, 1, 11)\n",
        "            # print(decoder_out.shape) #loop ends here\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1) #(N, 10, 11)\n",
        "        return outputs\n",
        "\n",
        "    def total_parameters(self):\n",
        "        return sum([p.numel() for p in self.parameters()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(output_len=10, use_gpu=False)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "eid8pK17iGnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le_TKOMvnzcA"
      },
      "source": [
        "print('model size is %.3f KB' % (model.total_parameters() * 4 / 1024))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function for calculating accuracy"
      ],
      "metadata": {
        "id": "QqIu8uIPPAyF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGG0NBQGoKBI"
      },
      "source": [
        "def calc_accuracy(pred, answer):\n",
        "    pred = np.argmax(pred, axis=2)\n",
        "    answer = np.argmax(answer, axis=2)\n",
        "    correct = (pred == answer).astype(int)\n",
        "    accuracy = correct.sum() / (pred.shape[0] * pred.shape[1])\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function for training the attention network"
      ],
      "metadata": {
        "id": "05WkQY-GPGPE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGt063tMujuu"
      },
      "source": [
        "def train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=False):\n",
        "    pbar = tqdm(total=len(dataloader), bar_format='{l_bar}{r_bar}', dynamic_ncols=True)\n",
        "    pbar.set_description(f'Epoch %d' % epoch)\n",
        "\n",
        "    for step, (batch_x, batch_y, _) in enumerate(dataloader):\n",
        "        if use_gpu:\n",
        "            batch_x = batch_x.cuda()\n",
        "            batch_y = batch_y.cuda()\n",
        "        pred = model(batch_x)\n",
        "        accuracy = calc_accuracy(pred.detach().cpu().numpy(), batch_y.detach().cpu().numpy())\n",
        "        loss = loss_fn(pred, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix(**{'loss':loss.detach().cpu().item(), 'accuracy':accuracy})\n",
        "        pbar.update()\n",
        "    save_checkpoint('./checkpoint', epoch, model, optimizer)\n",
        "\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model checkpoints"
      ],
      "metadata": {
        "id": "LSjpImUBPPGY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9557DeNwnb9"
      },
      "source": [
        "# For each epoch save the model and load the last epoch checkpoint\n",
        "def find_last_checkpoint(checkpoint_dir):\n",
        "    epochs = []\n",
        "    for name in os.listdir(checkpoint_dir):\n",
        "        if os.path.splitext(name)[-1] == '.pth':\n",
        "            epochs += [int(name.strip('ckpt_epoch_.pth'))]\n",
        "    if len(epochs) == 0:\n",
        "        raise IOError('no checkpoint found in {}'.format(checkpoint_dir))\n",
        "    return max(epochs)\n",
        "\n",
        "# Save the checkpoint at each epoch\n",
        "def save_checkpoint(checkpoint_dir, epoch, model, optimizer=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint['epoch'] = epoch\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model_state_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_state_dict = model.state_dict()\n",
        "    checkpoint['model'] = model_state_dict\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer_state_dict = optimizer.state_dict()\n",
        "\n",
        "        checkpoint['optimizer'] = optimizer_state_dict\n",
        "    else:\n",
        "        checkpoint['optimizer'] = None\n",
        "\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'ckpt_epoch_%02d.pth'% epoch))\n",
        "\n",
        "# Load the saved checkpoint\n",
        "def load_checkpoint(checkpoint_dir, epoch=-1):\n",
        "    if epoch == -1:\n",
        "        epoch = find_last_checkpoint(checkpoint_dir)\n",
        "    checkpoint_name = 'ckpt_epoch_%02d.pth'% epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    return ckpt\n",
        "\n",
        "# Saving the trained model\n",
        "def save_model(checkpoint_dir, epoch, model):\n",
        "    save_checkpoint(checkpoint_dir, epoch, model, optimizer=None)\n",
        "\n",
        "# Loading the model\n",
        "def load_model(checkpoint_dir, epoch, model):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        model_state_dict = ckpt['model']\n",
        "\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model.module.load_state_dict(model_state_dict)\n",
        "\n",
        "        else:\n",
        "            model.load_state_dict(model_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load model, {}'.format(e))\n",
        "    return model\n",
        "\n",
        "# Load the optimizer as a state dictionary\n",
        "def load_optimizer(checkpoint_dir, epoch, optimizer):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        optimizer_state_dict = ckpt['optimizer']\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load optimizer, {}'.format(e))\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "SnS1H4lvPW0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"checkpoint\"):\n",
        "  os.mkdir(\"checkpoint\")"
      ],
      "metadata": {
        "id": "XR98aIea0T6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8uDfNWb6IX"
      },
      "source": [
        "def main(dataloader, gpu_id=None,):\n",
        "\n",
        "    dataloader = dataloader\n",
        "\n",
        "    model = Model(output_len=10, use_gpu=True if gpu_id is not None else False)\n",
        "    if gpu_id is not None:\n",
        "        print('use gpu')\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "        n_gpus = torch.cuda.device_count()\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Mean Square Error loss between actual and predictions\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    model = load_model('./checkpoint', -1, model)\n",
        "    optimizer = load_optimizer('./checkpoint', -1, optimizer)\n",
        "\n",
        "    try:\n",
        "        trained_epoch = find_last_checkpoint('./checkpoint')\n",
        "        print('train form epoch %d' % (trained_epoch + 1))\n",
        "    except Exception as e:\n",
        "        print('train from the very begining, {}'.format(e))\n",
        "        trained_epoch = -1\n",
        "    for epoch in range(trained_epoch+1, 10):\n",
        "        train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=True if gpu_id is not None else False)\n",
        "\n",
        "\n",
        "if len(sys.argv) == 1:\n",
        "    main(dataloader,gpu_id='0')\n",
        "else:\n",
        "    main(dataloader,gpu_id=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model with 10000 test examples"
      ],
      "metadata": {
        "id": "eOTJwvplDeYj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFIuxaibl95S"
      },
      "source": [
        "n_datas = 10000\n",
        "dt_test = dates.iloc[40000:]\n",
        "dataset = Dataset(dates = dt_test,transform=transform, n_datas = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model\n",
        "#### Here we are evaluating the model and plotting the attention"
      ],
      "metadata": {
        "id": "XoGk_FcmDvdQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSm7Jf_pRnC"
      },
      "source": [
        "model = Model(output_len=10, use_gpu=False)\n",
        "try:\n",
        "    trained_epoch = find_last_checkpoint('./checkpoint')\n",
        "    print('load model %d' % (trained_epoch))\n",
        "except Exception as e:\n",
        "    print('no trained model found, {}'.format(e))\n",
        "\n",
        "model = load_model('./checkpoint', -1, model)\n",
        "\n",
        "# Set the model to eval mode\n",
        "model.eval()\n",
        "\n",
        "for i in range(1):\n",
        "  x, y, extra = dataset.__getitem__(i)\n",
        "\n",
        "  # Pass the predictions to the model\n",
        "  pred = model(torch.from_numpy(x).unsqueeze(0))\n",
        "\n",
        "  # Max probability of the predictions\n",
        "  pred = np.argmax(pred.detach().numpy(), axis=0)\n",
        "\n",
        "  # Map the vocab indexes back to the characters\n",
        "  pred = ''.join([dataset.inv_machine_vocab[p] for p in pred])\n",
        "\n",
        "  human_readable = extra['human_readable']\n",
        "  machine_readable = extra['machine_readable']\n",
        "\n",
        "  print('%s --> %s, true_value: %s' % (human_readable, pred, machine_readable))\n",
        "  # print(len(model.scores_for_paint), model.scores_for_paint[0].shape)\n",
        "  scores = np.array(model.scores_for_paint)\n",
        "\n",
        "  f = plt.figure(figsize=(8, 8.5))\n",
        "  ax = f.add_subplot(1, 1, 1)\n",
        "  i = ax.imshow(scores, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "  ax.set_xticks(range(10))\n",
        "  ax.set_xticklabels(human_readable[:10], rotation=0)\n",
        "\n",
        "  ax.set_yticks(range(10))\n",
        "  ax.set_yticklabels(machine_readable[:10], rotation=0)\n",
        "\n",
        "  plt.savefig('./attention.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the Accuracy"
      ],
      "metadata": {
        "id": "wne5QS_uEBxi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHFs3rYapWZ5"
      },
      "source": [
        "correct = 0\n",
        "for i in tqdm(range(10000)):\n",
        "  x, y, extra = dataset.__getitem__(i)\n",
        "\n",
        "  pred = model(torch.from_numpy(x).unsqueeze(0))\n",
        "\n",
        "  pred = np.argmax(pred.detach().numpy(), axis=0)\n",
        "\n",
        "  pred = ''.join([dataset.inv_machine_vocab[p] for p in pred])\n",
        "\n",
        "  human_readable = extra['human_readable']\n",
        "  machine_readable = answer = extra['machine_readable']\n",
        "\n",
        "  # print('%s --> %s, true_value: %s' % (human_readable, pred, machine_readable))\n",
        "  # print(len(model.scores_for_paint), model.scores_for_paint[0].shape)\n",
        "  scores = np.array(model.scores_for_paint)\n",
        "\n",
        "\n",
        "  # print(pred,answer)\n",
        "  if pred == answer:\n",
        "    correct += 1\n",
        "accuracy = correct / (10000)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Accuracy : {accuracy}\")"
      ],
      "metadata": {
        "id": "TJOeHCIK_eY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. The output of the attention model that serves as an input to the decoder is referred to as the  ____________________.\n",
        "Answer1 = \"\" #@param [\"\",\"attention sequence\",\"attention weights\",\"context vector\",\"reference input\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. How does the attention mechanism help the decoder while generating output?\n",
        "Answer2 = \"\" #@param [\"\",\"It allows the decoder to use only a limited amount of information from the input\",\"It allows the decoder to store additional state about past information in the sequence\",\"It allows the decoder to selectively pick important elements from the input sequence at each time instance\",\"It allows the decoder to forget irrelevant portions of the past information in the sequence\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}